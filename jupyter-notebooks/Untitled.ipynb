{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfad615e-fc52-42d4-9a82-5267ec0bdf48",
   "metadata": {},
   "source": [
    "# Session 3: Exploratory Data Analysis (EDA) and Pattern Discovery\n",
    "\n",
    "**Module:** Data Insights and Visualization  \n",
    "**Level:** 7 | **Credits:** 10  \n",
    "**Learning Outcomes Addressed:** LO1, LO3  \n",
    "**Big Academy Saudi Arabia - Riyadh Campus** üá∏üá¶\n",
    "\n",
    "---\n",
    "\n",
    "![Big Academy](https://img.shields.io/badge/Big%20Academy-Saudi%20Arabia-green?style=for-the-badge)\n",
    "![Level](https://img.shields.io/badge/Level-7%20Master's-blue?style=for-the-badge)\n",
    "![Session](https://img.shields.io/badge/Session-3%20of%208-orange?style=for-the-badge)\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "## üìã Session Overview\n",
    "\n",
    "Welcome to Session 3! Now that we have clean, prepared data from Session 2, we can begin the exciting journey of **Exploratory Data Analysis (EDA)**. This is where we discover hidden patterns, relationships, and insights that will drive business decisions. EDA is both an art and a science - combining statistical rigor with creative investigation.\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "#### By the end of this session, you will be able to:\n",
    "\n",
    "- **Calculate and interpret descriptive statistics for business insights**\n",
    "- **Discover relationships and correlations between variables**\n",
    "- **Analyze data distributions and identify patterns**\n",
    "- **Create professional visualizations for pattern discovery**\n",
    "- **Perform time series analysis and trend identification**\n",
    "- **Conduct comparative analysis and benchmarking**\n",
    "- **Apply statistical tests to validate findings**\n",
    "- **Generate executive-ready insights and recommendations**\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### üìö Learning Outcomes Alignment\n",
    "- **LO1:** Apply statistical and programming techniques to analyse complex structured and unstructured datasets\n",
    "- **LO3:** Critically interpret data patterns and trends for effective communication and decision-making\n",
    "\n",
    "**üìç Session 3 Focus: Pattern Discovery & Statistical Analysis**\n",
    "\n",
    "### ‚è±Ô∏è Session Structure (3 Hours)\n",
    "- **Part 1:** Descriptive Statistics & Data Summarization (45 minutes)\n",
    "- **Part 2:** Correlation Analysis & Relationships (45 minutes)  \n",
    "- **Part 3:** Distribution Analysis & Statistical Testing (45 minutes)\n",
    "- **Part 4:** Advanced Visualization for Pattern Discovery (30 minutes)\n",
    "- **Part 5:** Time Series Analysis & Trends (30 minutes)\n",
    "- **Part 6:** Comparative Analysis & Business Insights (15 minutes)\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "---\n",
    "## üìä Part 1: Descriptive Statistics and Data Summarization\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.1 Understanding Descriptive Statistics\n",
    "\n",
    "**What are Descriptive Statistics?**\n",
    "Descriptive statistics summarize and describe the main features of a dataset. They provide simple summaries about the data and help us understand the basic characteristics of our variables.\n",
    "\n",
    "**Why are they Important?**\n",
    "- Provide quick insights into data characteristics\n",
    "- Help identify data quality issues\n",
    "- Form the foundation for deeper analysis\n",
    "- Essential for business reporting and communication\n",
    "\n",
    "#### üìã Types of Descriptive Statistics\n",
    "\n",
    "| **Category** | **Measures** | **Purpose** | **When to Use** |\n",
    "|--------------|--------------|-------------|-----------------|\n",
    "| **Central Tendency** | Mean, Median, Mode | Find the \"center\" of data | Compare average performance, identify typical values |\n",
    "| **Dispersion** | Range, Variance, Standard Deviation | Measure data spread | Assess consistency, identify variability |\n",
    "| **Shape** | Skewness, Kurtosis | Describe distribution shape | Understand data distribution characteristics |\n",
    "| **Position** | Quartiles, Percentiles | Find data positions | Identify outliers, create benchmarks |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Session 3 Environment Setup\n",
    "print(\"üìä SESSION 3: EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "print(\"Big Academy Saudi Arabia - Level 7 Master's Program\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Session information\n",
    "session_info = {\n",
    "    \"Session\": \"3 - Exploratory Data Analysis\",\n",
    "    \"Duration\": \"3 hours\",\n",
    "    \"Focus\": \"LO1 & LO3\", \n",
    "    \"Key Skills\": \"Statistics, Visualization, Pattern Discovery\",\n",
    "    \"Tools\": \"Pandas, Matplotlib, Seaborn, SciPy\"\n",
    "}\n",
    "\n",
    "print(\"\\nüìã SESSION INFORMATION:\")\n",
    "for key, value in session_info.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for EDA!\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.2 Creating Sample Dataset for Analysis\n",
    "\n",
    "Let's create a comprehensive business dataset to practice our EDA techniques.\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Create comprehensive business dataset for EDA practice\n",
    "print(\"üèóÔ∏è CREATING COMPREHENSIVE BUSINESS DATASET FOR EDA:\\n\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate realistic e-commerce business data\n",
    "n_customers = 2000\n",
    "n_products = 50\n",
    "n_transactions = 8000\n",
    "\n",
    "# Customer data\n",
    "customer_ages = np.random.normal(35, 12, n_customers)\n",
    "customer_ages = np.clip(customer_ages, 18, 70).round().astype(int)\n",
    "\n",
    "customer_cities = np.random.choice(['Riyadh', 'Jeddah', 'Dammam', 'Mecca', 'Medina', 'Tabuk'], \n",
    "                                  n_customers, p=[0.35, 0.25, 0.15, 0.1, 0.1, 0.05])\n",
    "\n",
    "customer_segments = np.random.choice(['Premium', 'Standard', 'Basic'], \n",
    "                                   n_customers, p=[0.2, 0.5, 0.3])\n",
    "\n",
    "customers_df = pd.DataFrame({\n",
    "    'customer_id': range(1001, 1001 + n_customers),\n",
    "    'age': customer_ages,\n",
    "    'city': customer_cities,\n",
    "    'segment': customer_segments,\n",
    "    'registration_date': pd.date_range('2022-01-01', periods=n_customers, freq='6H')\n",
    "})\n",
    "\n",
    "# Product data  \n",
    "product_categories = ['Electronics', 'Fashion', 'Home', 'Sports', 'Books']\n",
    "product_prices = {\n",
    "    'Electronics': np.random.gamma(2, 300),\n",
    "    'Fashion': np.random.gamma(1.5, 150), \n",
    "    'Home': np.random.gamma(2, 200),\n",
    "    'Sports': np.random.gamma(1.8, 120),\n",
    "    'Books': np.random.gamma(1, 40)\n",
    "}\n",
    "\n",
    "products_df = pd.DataFrame({\n",
    "    'product_id': range(2001, 2001 + n_products),\n",
    "    'category': np.random.choice(product_categories, n_products),\n",
    "    'price': [product_prices[cat] for cat in np.random.choice(product_categories, n_products)]\n",
    "})\n",
    "\n",
    "# Transaction data\n",
    "transaction_dates = pd.date_range('2023-01-01', '2024-12-31', freq='H')\n",
    "selected_dates = np.random.choice(transaction_dates, n_transactions)\n",
    "\n",
    "# Create transactions with realistic patterns\n",
    "transactions_df = pd.DataFrame({\n",
    "    'transaction_id': range(5001, 5001 + n_transactions),\n",
    "    'customer_id': np.random.choice(customers_df['customer_id'], n_transactions),\n",
    "    'product_id': np.random.choice(products_df['product_id'], n_transactions),\n",
    "    'transaction_date': selected_dates,\n",
    "    'quantity': np.random.poisson(2, n_transactions) + 1,  # At least 1\n",
    "    'discount_percent': np.random.choice([0, 5, 10, 15, 20], n_transactions, p=[0.4, 0.25, 0.2, 0.1, 0.05])\n",
    "})\n",
    "\n",
    "# Merge to create comprehensive dataset\n",
    "ecommerce_data = transactions_df.merge(customers_df, on='customer_id', how='left')\n",
    "ecommerce_data = ecommerce_data.merge(products_df, on='product_id', how='left')\n",
    "\n",
    "# Calculate derived fields\n",
    "ecommerce_data['subtotal'] = ecommerce_data['price'] * ecommerce_data['quantity']\n",
    "ecommerce_data['discount_amount'] = ecommerce_data['subtotal'] * (ecommerce_data['discount_percent'] / 100)\n",
    "ecommerce_data['total_amount'] = ecommerce_data['subtotal'] - ecommerce_data['discount_amount']\n",
    "\n",
    "# Add time-based features\n",
    "ecommerce_data['month'] = ecommerce_data['transaction_date'].dt.month\n",
    "ecommerce_data['weekday'] = ecommerce_data['transaction_date'].dt.weekday\n",
    "ecommerce_data['hour'] = ecommerce_data['transaction_date'].dt.hour\n",
    "\n",
    "print(f\"‚úÖ Created comprehensive e-commerce dataset: {ecommerce_data.shape}\")\n",
    "print(f\"üìä Dataset covers: {len(customers_df)} customers, {len(products_df)} products, {len(transactions_df)} transactions\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample E-commerce Data:\")\n",
    "print(ecommerce_data[['customer_id', 'age', 'city', 'category', 'price', 'quantity', 'total_amount', 'transaction_date']].head(10))\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.3 Central Tendency Measures\n",
    "\n",
    "**What is Central Tendency?**\n",
    "Central tendency describes the center or typical value of a dataset. It helps us understand what a \"normal\" or \"average\" value looks like.\n",
    "\n",
    "#### üìä Central Tendency Methods Comparison\n",
    "\n",
    "| **Method** | **Description** | **When to Use** | **Advantages** | **Disadvantages** |\n",
    "|------------|-----------------|-----------------|----------------|-------------------|\n",
    "| **Mean** | Average of all values | Normal distributions, no extreme outliers | Easy to calculate, uses all data | Affected by outliers |\n",
    "| **Median** | Middle value when sorted | Skewed distributions, with outliers | Not affected by outliers | Ignores extreme values |\n",
    "| **Mode** | Most frequently occurring value | Categorical data, discrete values | Shows most common value | May not exist or be multiple |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Central Tendency Analysis\n",
    "print(\"üìä CENTRAL TENDENCY ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select numerical columns for analysis\n",
    "numerical_columns = ['age', 'price', 'quantity', 'total_amount']\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ MEAN (AVERAGE) ANALYSIS:\")\n",
    "print(\"   Purpose: Find the average value\")\n",
    "print(\"   Best for: Normal distributions, no extreme outliers\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    mean_value = ecommerce_data[col].mean()\n",
    "    print(f\"   ‚Ä¢ {col}: {mean_value:.2f}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ MEDIAN (MIDDLE VALUE) ANALYSIS:\")\n",
    "print(\"   Purpose: Find the middle value when data is sorted\")  \n",
    "print(\"   Best for: Skewed distributions, data with outliers\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    median_value = ecommerce_data[col].median()\n",
    "    print(f\"   ‚Ä¢ {col}: {median_value:.2f}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ MODE (MOST FREQUENT) ANALYSIS:\")\n",
    "print(\"   Purpose: Find the most commonly occurring value\")\n",
    "print(\"   Best for: Categorical data, discrete values\")\n",
    "\n",
    "# Mode for categorical columns\n",
    "categorical_columns = ['city', 'segment', 'category']\n",
    "for col in categorical_columns:\n",
    "    mode_value = ecommerce_data[col].mode().iloc[0] if len(ecommerce_data[col].mode()) > 0 else 'No mode'\n",
    "    print(f\"   ‚Ä¢ {col}: {mode_value}\")\n",
    "\n",
    "# Mode for numerical columns (rounded for meaningful results)\n",
    "for col in numerical_columns:\n",
    "    mode_value = ecommerce_data[col].round().mode()\n",
    "    if len(mode_value) > 0:\n",
    "        print(f\"   ‚Ä¢ {col}: {mode_value.iloc[0]}\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.4 Dispersion Measures\n",
    "\n",
    "**What is Dispersion?**\n",
    "Dispersion measures describe how spread out or scattered the data values are. They help us understand the variability and consistency in our data.\n",
    "\n",
    "#### üìä Dispersion Methods Comparison\n",
    "\n",
    "| **Method** | **Description** | **When to Use** | **Interpretation** |\n",
    "|------------|-----------------|-----------------|-------------------|\n",
    "| **Range** | Difference between max and min | Quick variability check | Larger range = more spread |\n",
    "| **Variance** | Average squared deviation from mean | Statistical calculations | Higher variance = more variability |\n",
    "| **Standard Deviation** | Square root of variance | Compare variability between datasets | Same units as original data |\n",
    "| **Coefficient of Variation** | Standard deviation / mean | Compare relative variability | Higher CV = more relative variability |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Dispersion Analysis\n",
    "print(\"\\nüìà DISPERSION (VARIABILITY) ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ RANGE ANALYSIS:\")\n",
    "print(\"   Purpose: Shows the spread between minimum and maximum values\")\n",
    "print(\"   Formula: Maximum - Minimum\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    min_val = ecommerce_data[col].min()\n",
    "    max_val = ecommerce_data[col].max()\n",
    "    range_val = max_val - min_val\n",
    "    print(f\"   ‚Ä¢ {col}: {range_val:.2f} (Min: {min_val:.2f}, Max: {max_val:.2f})\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ VARIANCE ANALYSIS:\")\n",
    "print(\"   Purpose: Measures average squared deviation from the mean\")\n",
    "print(\"   Interpretation: Higher variance = more scattered data\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    variance = ecommerce_data[col].var()\n",
    "    print(f\"   ‚Ä¢ {col}: {variance:.2f}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ STANDARD DEVIATION ANALYSIS:\")\n",
    "print(\"   Purpose: Shows typical deviation from the mean (same units as data)\")\n",
    "print(\"   Interpretation: ~68% of data falls within 1 std dev of mean\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    std_dev = ecommerce_data[col].std()\n",
    "    mean_val = ecommerce_data[col].mean()\n",
    "    print(f\"   ‚Ä¢ {col}: {std_dev:.2f} (Mean ¬± Std: {mean_val-std_dev:.2f} to {mean_val+std_dev:.2f})\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ COEFFICIENT OF VARIATION ANALYSIS:\")\n",
    "print(\"   Purpose: Relative variability (std dev / mean)\")\n",
    "print(\"   Interpretation: Higher CV = more relative variability\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    cv = (ecommerce_data[col].std() / ecommerce_data[col].mean()) * 100\n",
    "    print(f\"   ‚Ä¢ {col}: {cv:.1f}%\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.5 Distribution Shape Measures\n",
    "\n",
    "**What is Distribution Shape?**\n",
    "Distribution shape describes how the data values are distributed across the range. Understanding shape helps us choose appropriate analysis methods.\n",
    "\n",
    "#### üìä Shape Measures Comparison\n",
    "\n",
    "| **Measure** | **What it Shows** | **Values** | **Interpretation** |\n",
    "|-------------|-------------------|------------|-------------------|\n",
    "| **Skewness** | Asymmetry of distribution | < 0: Left-skewed, 0: Symmetric, > 0: Right-skewed | Shows direction of tail |\n",
    "| **Kurtosis** | Peakedness/tailedness | < 3: Light-tailed, 3: Normal, > 3: Heavy-tailed | Shows concentration around mean |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Distribution Shape Analysis\n",
    "print(\"\\nüìä DISTRIBUTION SHAPE ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ SKEWNESS ANALYSIS:\")\n",
    "print(\"   Purpose: Measures asymmetry of the distribution\")\n",
    "print(\"   Interpretation:\")\n",
    "print(\"     ‚Ä¢ Negative skew: Tail extends to the left (higher values more common)\")\n",
    "print(\"     ‚Ä¢ Zero skew: Symmetric distribution\")  \n",
    "print(\"     ‚Ä¢ Positive skew: Tail extends to the right (lower values more common)\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    skewness = ecommerce_data[col].skew()\n",
    "    if skewness < -0.5:\n",
    "        interpretation = \"Left-skewed (tail to left)\"\n",
    "    elif skewness > 0.5:\n",
    "        interpretation = \"Right-skewed (tail to right)\"\n",
    "    else:\n",
    "        interpretation = \"Approximately symmetric\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {col}: {skewness:.2f} ({interpretation})\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ KURTOSIS ANALYSIS:\")\n",
    "print(\"   Purpose: Measures peakedness and tail heaviness\")\n",
    "print(\"   Interpretation:\")\n",
    "print(\"     ‚Ä¢ High kurtosis: Sharp peak, heavy tails\")\n",
    "print(\"     ‚Ä¢ Low kurtosis: Flat peak, light tails\")\n",
    "print(\"     ‚Ä¢ Normal distribution kurtosis ‚âà 3\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    kurt = ecommerce_data[col].kurtosis()\n",
    "    if kurt > 3:\n",
    "        interpretation = \"Heavy-tailed (more extreme values)\"\n",
    "    elif kurt < 3:\n",
    "        interpretation = \"Light-tailed (fewer extreme values)\"\n",
    "    else:\n",
    "        interpretation = \"Normal-like tails\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {col}: {kurt:.2f} ({interpretation})\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.6 Position Measures (Percentiles and Quartiles)\n",
    "\n",
    "**What are Position Measures?**\n",
    "Position measures help us understand where specific values fall within the distribution and are useful for identifying outliers and creating benchmarks.\n",
    "\n",
    "#### üìä Position Measures Comparison\n",
    "\n",
    "| **Measure** | **Description** | **Business Use** | **Example Application** |\n",
    "|-------------|-----------------|------------------|------------------------|\n",
    "| **Quartiles** | Divide data into 4 equal parts | Performance ranking | Top 25% customers by sales |\n",
    "| **Percentiles** | Divide data into 100 equal parts | Detailed positioning | 95th percentile response time |\n",
    "| **Deciles** | Divide data into 10 equal parts | Performance segments | Top decile performers |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Position Measures Analysis\n",
    "print(\"\\nüìç POSITION MEASURES ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ QUARTILE ANALYSIS:\")\n",
    "print(\"   Purpose: Divides data into 4 equal parts\")\n",
    "print(\"   Q1 (25%): Bottom quarter, Q2 (50%): Median, Q3 (75%): Top quarter\")\n",
    "\n",
    "for col in numerical_columns:\n",
    "    q1 = ecommerce_data[col].quantile(0.25)\n",
    "    q2 = ecommerce_data[col].quantile(0.50)  # Same as median\n",
    "    q3 = ecommerce_data[col].quantile(0.75)\n",
    "    iqr = q3 - q1  # Interquartile Range\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {col}:\")\n",
    "    print(f\"     Q1 (25%): {q1:.2f}\")\n",
    "    print(f\"     Q2 (50%): {q2:.2f}\")  \n",
    "    print(f\"     Q3 (75%): {q3:.2f}\")\n",
    "    print(f\"     IQR: {iqr:.2f}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ PERCENTILE ANALYSIS:\")\n",
    "print(\"   Purpose: Shows value positions in the distribution\")\n",
    "print(\"   Useful for: Setting benchmarks, identifying top/bottom performers\")\n",
    "\n",
    "key_percentiles = [5, 10, 25, 50, 75, 90, 95]\n",
    "\n",
    "for col in numerical_columns[:2]:  # Show for first 2 columns to save space\n",
    "    print(f\"   ‚Ä¢ {col}:\")\n",
    "    for p in key_percentiles:\n",
    "        value = ecommerce_data[col].quantile(p/100)\n",
    "        print(f\"     {p}th percentile: {value:.2f}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ BUSINESS INSIGHTS FROM POSITION MEASURES:\")\n",
    "print(\"   Example: Customer Age Analysis\")\n",
    "\n",
    "# Customer age insights\n",
    "age_q1 = ecommerce_data['age'].quantile(0.25)\n",
    "age_q3 = ecommerce_data['age'].quantile(0.75)\n",
    "age_95th = ecommerce_data['age'].quantile(0.95)\n",
    "\n",
    "print(f\"   ‚Ä¢ 25% of customers are younger than {age_q1:.0f} years\")\n",
    "print(f\"   ‚Ä¢ 50% of customers are between {age_q1:.0f} and {age_q3:.0f} years\")  \n",
    "print(f\"   ‚Ä¢ Top 5% oldest customers are {age_95th:.0f}+ years\")\n",
    "\n",
    "print(\"\\n   Example: Sales Amount Analysis\")\n",
    "sales_q1 = ecommerce_data['total_amount'].quantile(0.25)\n",
    "sales_q3 = ecommerce_data['total_amount'].quantile(0.75)\n",
    "sales_90th = ecommerce_data['total_amount'].quantile(0.90)\n",
    "\n",
    "print(f\"   ‚Ä¢ 25% of transactions are under ${sales_q1:.2f}\")\n",
    "print(f\"   ‚Ä¢ 50% of transactions are between ${sales_q1:.2f} and ${sales_q3:.2f}\")\n",
    "print(f\"   ‚Ä¢ Top 10% highest transactions are ${sales_90th:.2f}+\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 1.7 Complete Descriptive Statistics Summary\n",
    "\n",
    "Now let's create a comprehensive summary using pandas built-in functions and interpret the results for business insights.\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Comprehensive Descriptive Statistics Summary\n",
    "print(\"\\nüìã COMPREHENSIVE DESCRIPTIVE STATISTICS SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ PANDAS DESCRIBE() FUNCTION:\")\n",
    "print(\"   Purpose: Provides complete statistical summary\")\n",
    "print(\"   Includes: Count, Mean, Std, Min, 25%, 50%, 75%, Max\")\n",
    "\n",
    "numerical_summary = ecommerce_data[numerical_columns].describe()\n",
    "print(\"\\nNumerical Variables Summary:\")\n",
    "print(numerical_summary.round(2))\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ CATEGORICAL VARIABLES SUMMARY:\")\n",
    "print(\"   Purpose: Shows frequency distributions for categorical data\")\n",
    "\n",
    "categorical_summary = ecommerce_data[categorical_columns].describe()\n",
    "print(\"\\nCategorical Variables Summary:\")\n",
    "print(categorical_summary)\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ DETAILED FREQUENCY ANALYSIS:\")\n",
    "print(\"   Purpose: Understanding distribution of categorical variables\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\n   {col.upper()} DISTRIBUTION:\")\n",
    "    freq_counts = ecommerce_data[col].value_counts()\n",
    "    freq_percentages = ecommerce_data[col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for category in freq_counts.index:\n",
    "        count = freq_counts[category]\n",
    "        percentage = freq_percentages[category]\n",
    "        print(f\"     ‚Ä¢ {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ BUSINESS INSIGHTS FROM DESCRIPTIVE STATISTICS:\")\n",
    "\n",
    "# Age insights\n",
    "avg_age = ecommerce_data['age'].mean()\n",
    "age_std = ecommerce_data['age'].std()\n",
    "print(f\"\\n   üë• CUSTOMER DEMOGRAPHICS:\")\n",
    "print(f\"   ‚Ä¢ Average customer age: {avg_age:.1f} years\")\n",
    "print(f\"   ‚Ä¢ Age diversity (std dev): {age_std:.1f} years\")\n",
    "print(f\"   ‚Ä¢ Most customers are between {avg_age-age_std:.0f}-{avg_age+age_std:.0f} years\")\n",
    "\n",
    "# Sales insights  \n",
    "avg_transaction = ecommerce_data['total_amount'].mean()\n",
    "median_transaction = ecommerce_data['total_amount'].median()\n",
    "print(f\"\\n   üí∞ SALES PATTERNS:\")\n",
    "print(f\"   ‚Ä¢ Average transaction: ${avg_transaction:.2f}\")\n",
    "print(f\"   ‚Ä¢ Median transaction: ${median_transaction:.2f}\")\n",
    "if avg_transaction > median_transaction:\n",
    "    print(f\"   ‚Ä¢ Distribution is right-skewed (few high-value transactions)\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Distribution is balanced or left-skewed\")\n",
    "\n",
    "# Product insights\n",
    "avg_price = ecommerce_data['price'].mean()\n",
    "price_cv = (ecommerce_data['price'].std() / avg_price) * 100\n",
    "print(f\"\\n   üõçÔ∏è PRODUCT PRICING:\")\n",
    "print(f\"   ‚Ä¢ Average product price: ${avg_price:.2f}\")\n",
    "print(f\"   ‚Ä¢ Price variability: {price_cv:.1f}% (Coefficient of Variation)\")\n",
    "if price_cv > 50:\n",
    "    print(f\"   ‚Ä¢ High price diversity across products\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Moderate price consistency across products\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "---\n",
    "## üîó Part 2: Correlation Analysis and Relationships\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 2.1 Understanding Correlation\n",
    "\n",
    "**What is Correlation?**\n",
    "Correlation measures the strength and direction of the linear relationship between two variables. It helps us understand how variables move together.\n",
    "\n",
    "**Important Note:** Correlation does NOT imply causation! Just because two variables are correlated doesn't mean one causes the other.\n",
    "\n",
    "#### üìä Correlation Methods Comparison\n",
    "\n",
    "| **Method** | **Type of Data** | **Range** | **When to Use** | **Interpretation** |\n",
    "|------------|------------------|-----------|-----------------|-------------------|\n",
    "| **Pearson** | Continuous, linear relationships | -1 to +1 | Normal distributions, linear relationships | Most common correlation measure |\n",
    "| **Spearman** | Ordinal, non-linear relationships | -1 to +1 | Non-normal distributions, monotonic relationships | Rank-based correlation |\n",
    "| **Kendall's Tau** | Ordinal, small samples | -1 to +1 | Small datasets, many tied ranks | More robust than Spearman |\n",
    "\n",
    "#### üéØ Correlation Strength Interpretation\n",
    "\n",
    "| **Correlation Value** | **Strength** | **Business Meaning** |\n",
    "|----------------------|--------------|---------------------|\n",
    "| 0.0 to 0.3 | Weak | Little relationship |\n",
    "| 0.3 to 0.7 | Moderate | Some relationship worth investigating |\n",
    "| 0.7 to 1.0 | Strong | Strong relationship, important for business |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Correlation Analysis\n",
    "print(\"üîó CORRELATION ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ PEARSON CORRELATION ANALYSIS:\")\n",
    "print(\"   Purpose: Measures linear relationships between numerical variables\")\n",
    "print(\"   Range: -1 (perfect negative) to +1 (perfect positive)\")\n",
    "print(\"   Requirements: Numerical data, linear relationships\")\n",
    "\n",
    "# Calculate Pearson correlation matrix\n",
    "correlation_matrix = ecommerce_data[numerical_columns].corr()\n",
    "\n",
    "print(\"\\nPearson Correlation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ INTERPRETING CORRELATION VALUES:\")\n",
    "print(\"   Strong correlations (|r| > 0.7):\")\n",
    "\n",
    "# Find strong correlations\n",
    "strong_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7:\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            strong_correlations.append((var1, var2, corr_value))\n",
    "\n",
    "if strong_correlations:\n",
    "    for var1, var2, corr in strong_correlations:\n",
    "        direction = \"Positive\" if corr > 0 else \"Negative\"\n",
    "        print(f\"   ‚Ä¢ {var1} ‚Üî {var2}: {corr:.3f} ({direction})\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No strong correlations found\")\n",
    "\n",
    "print(\"\\n   Moderate correlations (0.3 < |r| < 0.7):\")\n",
    "moderate_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if 0.3 < abs(corr_value) < 0.7:\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            moderate_correlations.append((var1, var2, corr_value))\n",
    "\n",
    "if moderate_correlations:\n",
    "    for var1, var2, corr in moderate_correlations:\n",
    "        direction = \"Positive\" if corr > 0 else \"Negative\"\n",
    "        print(f\"   ‚Ä¢ {var1} ‚Üî {var2}: {corr:.3f} ({direction})\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No moderate correlations found\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Spearman Correlation (Rank-based)\n",
    "print(\"\\n3Ô∏è‚É£ SPEARMAN CORRELATION ANALYSIS:\")\n",
    "print(\"   Purpose: Measures monotonic relationships (not necessarily linear)\")\n",
    "print(\"   Advantage: Works with non-normal data and non-linear relationships\")\n",
    "print(\"   Method: Based on rank ordering rather than actual values\")\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr = ecommerce_data[numerical_columns].corr(method='spearman')\n",
    "\n",
    "print(\"\\nSpearman Correlation Matrix:\")\n",
    "print(spearman_corr.round(3))\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ COMPARING PEARSON vs SPEARMAN:\")\n",
    "print(\"   Purpose: Identify non-linear relationships\")\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        pearson_val = correlation_matrix.iloc[i, j]\n",
    "        spearman_val = spearman_corr.iloc[i, j]\n",
    "        \n",
    "        if abs(pearson_val - spearman_val) > 0.1:  # Significant difference\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            print(f\"   ‚Ä¢ {var1} ‚Üî {var2}:\")\n",
    "            print(f\"     Pearson: {pearson_val:.3f}, Spearman: {spearman_val:.3f}\")\n",
    "            print(f\"     ‚Üí Suggests non-linear relationship\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ BUSINESS INSIGHTS FROM CORRELATIONS:\")\n",
    "\n",
    "# Age and spending correlation\n",
    "age_spending_corr = ecommerce_data['age'].corr(ecommerce_data['total_amount'])\n",
    "print(f\"\\n   üë• Age vs Spending: {age_spending_corr:.3f}\")\n",
    "if abs(age_spending_corr) > 0.3:\n",
    "    direction = \"increases\" if age_spending_corr > 0 else \"decreases\"\n",
    "    print(f\"   ‚Üí Customer spending {direction} with age\")\n",
    "else:\n",
    "    print(f\"   ‚Üí Age has little relationship with spending amount\")\n",
    "\n",
    "# Price and quantity correlation  \n",
    "price_quantity_corr = ecommerce_data['price'].corr(ecommerce_data['quantity'])\n",
    "print(f\"\\n   üí∞ Price vs Quantity: {price_quantity_corr:.3f}\")\n",
    "if abs(price_quantity_corr) > 0.3:\n",
    "    direction = \"increases\" if price_quantity_corr > 0 else \"decreases\"\n",
    "    print(f\"   ‚Üí Quantity purchased {direction} with product price\")\n",
    "else:\n",
    "    print(f\"   ‚Üí Product price has little impact on quantity purchased\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 2.2 Correlation Visualization\n",
    "\n",
    "Visual correlation analysis helps identify patterns that might not be obvious from numbers alone.\n",
    "\n",
    "#### üìä Correlation Visualization Methods\n",
    "\n",
    "| **Method** | **Best For** | **Advantages** | **When to Use** |\n",
    "|------------|--------------|----------------|-----------------|\n",
    "| **Heatmap** | Overview of all correlations | Shows patterns across many variables | Initial correlation exploration |\n",
    "| **Scatter Plot** | Individual variable relationships | Shows relationship shape and outliers | Detailed analysis of specific pairs |\n",
    "| **Pair Plot** | Multiple variable relationships | Shows distributions + correlations | Comprehensive relationship analysis |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Correlation Visualization\n",
    "print(\"\\nüìä CORRELATION VISUALIZATION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ CORRELATION HEATMAP:\")\n",
    "print(\"   Purpose: Visual overview of all correlations\")\n",
    "print(\"   Benefits: Quick identification of strong relationships\")\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Correlation Heatmap - Numerical Variables', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Heatmap created - Look for dark red (positive) and dark blue (negative) cells\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ SCATTER PLOT ANALYSIS:\")\n",
    "print(\"   Purpose: Detailed view of individual relationships\")\n",
    "print(\"   Benefits: Shows relationship shape, outliers, and data distribution\")\n",
    "\n",
    "# Create scatter plots for interesting relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Scatter Plot Analysis - Key Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Age vs Total Amount\n",
    "axes[0, 0].scatter(ecommerce_data['age'], ecommerce_data['total_amount'], alpha=0.6, color='blue')\n",
    "axes[0, 0].set_xlabel('Customer Age')\n",
    "axes[0, 0].set_ylabel('Total Amount ($)')\n",
    "axes[0, 0].set_title('Age vs Total Amount')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(ecommerce_data['age'], ecommerce_data['total_amount'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(ecommerce_data['age'], p(ecommerce_data['age']), \"r--\", alpha=0.8)\n",
    "\n",
    "# Plot 2: Price vs Quantity\n",
    "axes[0, 1].scatter(ecommerce_data['price'], ecommerce_data['quantity'], alpha=0.6, color='green')\n",
    "axes[0, 1].set_xlabel('Product Price ($)')\n",
    "axes[0, 1].set_ylabel('Quantity Purchased')\n",
    "axes[0, 1].set_title('Price vs Quantity')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Price vs Total Amount\n",
    "axes[1, 0].scatter(ecommerce_data['price'], ecommerce_data['total_amount'], alpha=0.6, color='orange')\n",
    "axes[1, 0].set_xlabel('Product Price ($)')\n",
    "axes[1, 0].set_ylabel('Total Amount ($)')\n",
    "axes[1, 0].set_title('Price vs Total Amount')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Quantity vs Total Amount\n",
    "axes[1, 1].scatter(ecommerce_data['quantity'], ecommerce_data['total_amount'], alpha=0.6, color='purple')\n",
    "axes[1, 1].set_xlabel('Quantity Purchased')\n",
    "axes[1, 1].set_ylabel('Total Amount ($)')\n",
    "axes[1, 1].set_title('Quantity vs Total Amount')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Scatter plots created - Look for linear patterns, clusters, and outliers\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 2.3 Categorical Variable Relationships\n",
    "\n",
    "Understanding relationships between categorical variables requires different methods than numerical correlations.\n",
    "\n",
    "#### üìä Categorical Relationship Methods\n",
    "\n",
    "| **Method** | **Purpose** | **When to Use** | **Output** |\n",
    "|------------|-------------|-----------------|------------|\n",
    "| **Cross-tabulation** | Frequency relationships | Two categorical variables | Counts and percentages |\n",
    "| **Chi-square test** | Statistical significance | Test independence | P-value and test statistic |\n",
    "| **Cram√©r's V** | Association strength | Strength of relationship | 0 to 1 (stronger = higher) |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Categorical Variable Relationships\n",
    "print(\"\\nüè∑Ô∏è CATEGORICAL VARIABLE RELATIONSHIPS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ CROSS-TABULATION ANALYSIS:\")\n",
    "print(\"   Purpose: Shows frequency distribution between categorical variables\")\n",
    "print(\"   Benefits: Understand how categories relate to each other\")\n",
    "\n",
    "# Cross-tabulation: City vs Segment\n",
    "print(\"\\n   CUSTOMER CITY vs SEGMENT:\")\n",
    "city_segment_crosstab = pd.crosstab(ecommerce_data['city'], ecommerce_data['segment'])\n",
    "print(city_segment_crosstab)\n",
    "\n",
    "# Add percentages\n",
    "print(\"\\n   Percentage distribution (by city):\")\n",
    "city_segment_percent = pd.crosstab(ecommerce_data['city'], ecommerce_data['segment'], normalize='index') * 100\n",
    "print(city_segment_percent.round(1))\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ CATEGORY vs NUMERICAL ANALYSIS:\")\n",
    "print(\"   Purpose: How categorical variables relate to numerical outcomes\")\n",
    "\n",
    "# Segment vs Average spending\n",
    "print(\"\\n   CUSTOMER SEGMENT vs AVERAGE SPENDING:\")\n",
    "segment_spending = ecommerce_data.groupby('segment')['total_amount'].agg(['mean', 'median', 'count'])\n",
    "segment_spending.columns = ['Average_Spending', 'Median_Spending', 'Number_of_Transactions']\n",
    "print(segment_spending.round(2))\n",
    "\n",
    "# City vs Average spending\n",
    "print(\"\\n   CUSTOMER CITY vs AVERAGE SPENDING:\")\n",
    "city_spending = ecommerce_data.groupby('city')['total_amount'].agg(['mean', 'median', 'count'])\n",
    "city_spending.columns = ['Average_Spending', 'Median_Spending', 'Number_of_Transactions']\n",
    "print(city_spending.round(2))\n",
    "\n",
    "# Product category vs metrics\n",
    "print(\"\\n   PRODUCT CATEGORY vs METRICS:\")\n",
    "category_metrics = ecommerce_data.groupby('category').agg({\n",
    "    'total_amount': ['mean', 'median'],\n",
    "    'quantity': 'mean',\n",
    "    'price': 'mean'\n",
    "}).round(2)\n",
    "print(category_metrics)\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ CHI-SQUARE TEST OF INDEPENDENCE:\")\n",
    "print(\"   Purpose: Test if two categorical variables are independent\")\n",
    "print(\"   Null hypothesis: Variables are independent (not related)\")\n",
    "print(\"   Alternative: Variables are dependent (related)\")\n",
    "\n",
    "# Chi-square test for city vs segment\n",
    "chi2_stat, p_value, dof, expected = stats.chi2_contingency(city_segment_crosstab)\n",
    "\n",
    "print(f\"\\n   CITY vs SEGMENT Chi-square test:\")\n",
    "print(f\"   ‚Ä¢ Chi-square statistic: {chi2_stat:.3f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value:.3f}\")\n",
    "print(f\"   ‚Ä¢ Degrees of freedom: {dof}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"   ‚Üí Significant relationship (p < 0.05): City and segment are related\")\n",
    "else:\n",
    "    print(f\"   ‚Üí No significant relationship (p >= 0.05): City and segment appear independent\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ BUSINESS INSIGHTS FROM CATEGORICAL RELATIONSHIPS:\")\n",
    "\n",
    "# Find highest spending segment\n",
    "highest_segment = segment_spending['Average_Spending'].idxmax()\n",
    "highest_amount = segment_spending['Average_Spending'].max()\n",
    "print(f\"\\n   üíé Highest value segment: {highest_segment} (${highest_amount:.2f} average)\")\n",
    "\n",
    "# Find best performing city\n",
    "best_city = city_spending['Average_Spending'].idxmax()\n",
    "best_city_amount = city_spending['Average_Spending'].max()\n",
    "print(f\"   üèôÔ∏è Best performing city: {best_city} (${best_city_amount:.2f} average)\")\n",
    "\n",
    "# Find most popular category\n",
    "category_popularity = ecommerce_data['category'].value_counts()\n",
    "most_popular = category_popularity.index[0]\n",
    "most_popular_count = category_popularity.iloc[0]\n",
    "print(f\"   üõçÔ∏è Most popular category: {most_popular} ({most_popular_count:,} transactions)\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "---\n",
    "## üìà Part 3: Distribution Analysis and Statistical Testing\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 3.1 Understanding Data Distributions\n",
    "\n",
    "**What is a Distribution?**\n",
    "A distribution shows how values of a variable are spread across different ranges. Understanding distributions helps us choose appropriate statistical methods and identify patterns.\n",
    "\n",
    "#### üìä Common Distribution Types\n",
    "\n",
    "| **Distribution** | **Shape** | **Characteristics** | **Examples** | **Statistical Tests** |\n",
    "|------------------|-----------|--------------------|--------------|--------------------|\n",
    "| **Normal** | Bell-shaped, symmetric | Mean = Median = Mode | Heights, test scores | t-test, ANOVA |\n",
    "| **Right-skewed** | Tail extends right | Mean > Median | Income, sales amounts | Non-parametric tests |\n",
    "| **Left-skewed** | Tail extends left | Mean < Median | Age at retirement | Non-parametric tests |\n",
    "| **Uniform** | Flat, all values equal | No clear central tendency | Random numbers | Chi-square tests |\n",
    "| **Bimodal** | Two peaks | Two common value ranges | Mixed populations | Mixture analysis |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Distribution Analysis\n",
    "print(\"üìà DISTRIBUTION ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ VISUAL DISTRIBUTION ANALYSIS:\")\n",
    "print(\"   Purpose: Understand the shape and characteristics of data distributions\")\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Distribution Analysis - Key Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Age Distribution\n",
    "axes[0, 0].hist(ecommerce_data['age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(ecommerce_data['age'].mean(), color='red', linestyle='--', label=f'Mean: {ecommerce_data[\"age\"].mean():.1f}')\n",
    "axes[0, 0].axvline(ecommerce_data['age'].median(), color='green', linestyle='--', label=f'Median: {ecommerce_data[\"age\"].median():.1f}')\n",
    "axes[0, 0].set_xlabel('Customer Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Total Amount Distribution\n",
    "axes[0, 1].hist(ecommerce_data['total_amount'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].axvline(ecommerce_data['total_amount'].mean(), color='red', linestyle='--', label=f'Mean: ${ecommerce_data[\"total_amount\"].mean():.0f}')\n",
    "axes[0, 1].axvline(ecommerce_data['total_amount'].median(), color='green', linestyle='--', label=f'Median: ${ecommerce_data[\"total_amount\"].median():.0f}')\n",
    "axes[0, 1].set_xlabel('Total Amount ($)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Total Amount Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Price Distribution\n",
    "axes[1, 0].hist(ecommerce_data['price'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1, 0].axvline(ecommerce_data['price'].mean(), color='red', linestyle='--', label=f'Mean: ${ecommerce_data[\"price\"].mean():.0f}')\n",
    "axes[1, 0].axvline(ecommerce_data['price'].median(), color='green', linestyle='--', label=f'Median: ${ecommerce_data[\"price\"].median():.0f}')\n",
    "axes[1, 0].set_xlabel('Product Price ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Product Price Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Quantity Distribution\n",
    "axes[1, 1].hist(ecommerce_data['quantity'], bins=range(1, ecommerce_data['quantity'].max()+2), \n",
    "                alpha=0.7, color='lightyellow', edgecolor='black')\n",
    "axes[1, 1].axvline(ecommerce_data['quantity'].mean(), color='red', linestyle='--', label=f'Mean: {ecommerce_data[\"quantity\"].mean():.1f}')\n",
    "axes[1, 1].axvline(ecommerce_data['quantity'].median(), color='green', linestyle='--', label=f'Median: {ecommerce_data[\"quantity\"].median():.1f}')\n",
    "axes[1, 1].set_xlabel('Quantity Purchased')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Quantity Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Distribution plots created\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ DISTRIBUTION SHAPE ANALYSIS:\")\n",
    "print(\"   Purpose: Classify distribution shapes for appropriate analysis methods\")\n",
    "\n",
    "distributions = ['age', 'total_amount', 'price', 'quantity']\n",
    "\n",
    "for var in distributions:\n",
    "    mean_val = ecommerce_data[var].mean()\n",
    "    median_val = ecommerce_data[var].median()\n",
    "    skewness = ecommerce_data[var].skew()\n",
    "    \n",
    "    # Determine distribution shape\n",
    "    if abs(skewness) < 0.5:\n",
    "        shape = \"Approximately Normal\"\n",
    "    elif skewness > 0.5:\n",
    "        shape = \"Right-skewed (Positive skew)\"\n",
    "    else:\n",
    "        shape = \"Left-skewed (Negative skew)\"\n",
    "    \n",
    "    # Compare mean and median\n",
    "    if abs(mean_val - median_val) / median_val < 0.1:\n",
    "        central_tendency = \"Mean ‚âà Median (symmetric)\"\n",
    "    elif mean_val > median_val:\n",
    "        central_tendency = \"Mean > Median (right tail)\"\n",
    "    else:\n",
    "        central_tendency = \"Mean < Median (left tail)\"\n",
    "    \n",
    "    print(f\"\\n   üìä {var.upper()}:\")\n",
    "    print(f\"      Shape: {shape}\")\n",
    "    print(f\"      Central tendency: {central_tendency}\")\n",
    "    print(f\"      Skewness: {skewness:.3f}\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 3.2 Normality Testing\n",
    "\n",
    "Testing whether data follows a normal distribution is crucial for choosing appropriate statistical methods.\n",
    "\n",
    "#### üìä Normality Tests Comparison\n",
    "\n",
    "| **Test** | **Best For** | **Sample Size** | **Null Hypothesis** | **Decision Rule** |\n",
    "|----------|--------------|-----------------|-------------------|-------------------|\n",
    "| **Shapiro-Wilk** | Small to medium samples | < 5000 | Data is normally distributed | p > 0.05 = Normal |\n",
    "| **Anderson-Darling** | Any sample size | Any | Data is normally distributed | p > 0.05 = Normal |\n",
    "| **Kolmogorov-Smirnov** | Large samples | > 2000 | Data is normally distributed | p > 0.05 = Normal |\n",
    "| **D'Agostino** | Large samples | > 20 | Data is normally distributed | p > 0.05 = Normal |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Normality Testing\n",
    "print(\"\\nüîç NORMALITY TESTING:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ SHAPIRO-WILK TEST:\")\n",
    "print(\"   Purpose: Test if data comes from a normal distribution\")\n",
    "print(\"   Best for: Small to medium samples (< 5000)\")\n",
    "print(\"   Null hypothesis: Data is normally distributed\")\n",
    "print(\"   Decision: p > 0.05 suggests normal distribution\")\n",
    "\n",
    "for var in distributions:\n",
    "    # Take sample for Shapiro-Wilk (works best with smaller samples)\n",
    "    sample_data = ecommerce_data[var].dropna().sample(min(1000, len(ecommerce_data[var])))\n",
    "    \n",
    "    stat, p_value = stats.shapiro(sample_data)\n",
    "    \n",
    "    interpretation = \"Normally distributed\" if p_value > 0.05 else \"Not normally distributed\"\n",
    "    \n",
    "    print(f\"\\n   {var.upper()}:\")\n",
    "    print(f\"   ‚Ä¢ Statistic: {stat:.6f}\")\n",
    "    print(f\"   ‚Ä¢ P-value: {p_value:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Result: {interpretation}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ ANDERSON-DARLING TEST:\")\n",
    "print(\"   Purpose: More powerful test for normality\")\n",
    "print(\"   Advantage: Works well with any sample size\")\n",
    "\n",
    "for var in distributions:\n",
    "    result = stats.anderson(ecommerce_data[var].dropna(), dist='norm')\n",
    "    \n",
    "    print(f\"\\n   {var.upper()}:\")\n",
    "    print(f\"   ‚Ä¢ Statistic: {result.statistic:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Critical values: {result.critical_values}\")\n",
    "    print(f\"   ‚Ä¢ Significance levels: {result.significance_level}%\")\n",
    "    \n",
    "    # Check against 5% significance level\n",
    "    critical_5 = result.critical_values[2]  # 5% level is usually index 2\n",
    "    if result.statistic < critical_5:\n",
    "        print(f\"   ‚Ä¢ Result: Normally distributed (at 5% level)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Result: Not normally distributed (at 5% level)\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Q-Q PLOT ANALYSIS:\")\n",
    "print(\"   Purpose: Visual assessment of normality\")\n",
    "print(\"   Interpretation: Points on straight line = normal distribution\")\n",
    "\n",
    "# Create Q-Q plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Q-Q Plots for Normality Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "variables_for_qq = ['age', 'total_amount', 'price', 'quantity']\n",
    "plot_positions = [(0,0), (0,1), (1,0), (1,1)]\n",
    "\n",
    "for i, var in enumerate(variables_for_qq):\n",
    "    row, col = plot_positions[i]\n",
    "    stats.probplot(ecommerce_data[var].dropna(), dist=\"norm\", plot=axes[row, col])\n",
    "    axes[row, col].set_title(f'Q-Q Plot: {var.title()}')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Q-Q plots created - Straight line indicates normal distribution\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ BUSINESS IMPLICATIONS OF NORMALITY:\")\n",
    "\n",
    "print(\"\\n   üìä STATISTICAL METHOD RECOMMENDATIONS:\")\n",
    "normal_vars = []\n",
    "non_normal_vars = []\n",
    "\n",
    "for var in distributions:\n",
    "    sample_data = ecommerce_data[var].dropna().sample(min(1000, len(ecommerce_data[var])))\n",
    "    _, p_value = stats.shapiro(sample_data)\n",
    "    \n",
    "    if p_value > 0.05:\n",
    "        normal_vars.append(var)\n",
    "    else:\n",
    "        non_normal_vars.append(var)\n",
    "\n",
    "if normal_vars:\n",
    "    print(f\"\\n   ‚úÖ Normal distributions: {', '.join(normal_vars)}\")\n",
    "    print(f\"      ‚Üí Use: t-tests, ANOVA, Pearson correlation\")\n",
    "\n",
    "if non_normal_vars:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Non-normal distributions: {', '.join(non_normal_vars)}\")\n",
    "    print(f\"      ‚Üí Use: Mann-Whitney U, Kruskal-Wallis, Spearman correlation\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 3.3 Statistical Hypothesis Testing\n",
    "\n",
    "Hypothesis testing helps us make data-driven decisions by testing specific claims about our data.\n",
    "\n",
    "#### üìä Common Statistical Tests Comparison\n",
    "\n",
    "| **Test** | **Purpose** | **Data Requirements** | **Example Question** |\n",
    "|----------|-------------|----------------------|---------------------|\n",
    "| **One-sample t-test** | Compare mean to target value | Normal distribution, continuous | Is average age = 30? |\n",
    "| **Two-sample t-test** | Compare means of two groups | Normal distribution, continuous | Do men spend more than women? |\n",
    "| **Mann-Whitney U** | Compare two groups (non-parametric) | Ordinal data, any distribution | Do segments differ in spending? |\n",
    "| **Chi-square test** | Test categorical associations | Categorical data, adequate sample | Are city and segment related? |\n",
    "| **ANOVA** | Compare multiple group means | Normal distribution, continuous | Do all cities have same average spending? |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Statistical Hypothesis Testing\n",
    "print(\"\\nüß™ STATISTICAL HYPOTHESIS TESTING:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ ONE-SAMPLE T-TEST:\")\n",
    "print(\"   Purpose: Test if a sample mean equals a specific value\")\n",
    "print(\"   Example: Is the average customer age significantly different from 35?\")\n",
    "\n",
    "# Test if average age is significantly different from 35\n",
    "age_data = ecommerce_data['age'].dropna()\n",
    "target_age = 35\n",
    "\n",
    "t_stat, p_value = stats.ttest_1samp(age_data, target_age)\n",
    "\n",
    "print(f\"\\n   HYPOTHESIS TEST:\")\n",
    "print(f\"   ‚Ä¢ Null hypothesis: Average age = {target_age}\")\n",
    "print(f\"   ‚Ä¢ Alternative hypothesis: Average age ‚â† {target_age}\")\n",
    "print(f\"   ‚Ä¢ Sample mean: {age_data.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ T-statistic: {t_stat:.4f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    direction = \"higher\" if age_data.mean() > target_age else \"lower\"\n",
    "    print(f\"   ‚Ä¢ Result: Significant difference - Average age is significantly {direction} than {target_age}\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Result: No significant difference - Average age is not significantly different from {target_age}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ TWO-SAMPLE T-TEST:\")\n",
    "print(\"   Purpose: Compare means between two groups\")\n",
    "print(\"   Example: Do Premium and Basic customers spend differently?\")\n",
    "\n",
    "# Compare spending between Premium and Basic customers\n",
    "premium_spending = ecommerce_data[ecommerce_data['segment'] == 'Premium']['total_amount']\n",
    "basic_spending = ecommerce_data[ecommerce_data['segment'] == 'Basic']['total_amount']\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(premium_spending.dropna(), basic_spending.dropna())\n",
    "\n",
    "print(f\"\\n   GROUP COMPARISON:\")\n",
    "print(f\"   ‚Ä¢ Premium customers mean: ${premium_spending.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Basic customers mean: ${basic_spending.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Difference: ${premium_spending.mean() - basic_spending.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ T-statistic: {t_stat:.4f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    higher_group = \"Premium\" if premium_spending.mean() > basic_spending.mean() else \"Basic\"\n",
    "    print(f\"   ‚Ä¢ Result: Significant difference - {higher_group} customers spend significantly more\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Result: No significant difference in spending between Premium and Basic customers\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ MANN-WHITNEY U TEST (NON-PARAMETRIC):\")\n",
    "print(\"   Purpose: Compare two groups when data is not normally distributed\")\n",
    "print(\"   Example: Compare spending between two cities\")\n",
    "\n",
    "riyadh_spending = ecommerce_data[ecommerce_data['city'] == 'Riyadh']['total_amount']\n",
    "jeddah_spending = ecommerce_data[ecommerce_data['city'] == 'Jeddah']['total_amount']\n",
    "\n",
    "u_stat, p_value = stats.mannwhitneyu(riyadh_spending.dropna(), jeddah_spending.dropna(), alternative='two-sided')\n",
    "\n",
    "print(f\"\\n   CITY COMPARISON (NON-PARAMETRIC):\")\n",
    "print(f\"   ‚Ä¢ Riyadh median: ${riyadh_spending.median():.2f}\")\n",
    "print(f\"   ‚Ä¢ Jeddah median: ${jeddah_spending.median():.2f}\")\n",
    "print(f\"   ‚Ä¢ U-statistic: {u_stat:.4f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    higher_city = \"Riyadh\" if riyadh_spending.median() > jeddah_spending.median() else \"Jeddah\"\n",
    "    print(f\"   ‚Ä¢ Result: Significant difference - {higher_city} has significantly higher spending\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Result: No significant difference in spending between Riyadh and Jeddah\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ ANOVA (ANALYSIS OF VARIANCE):\")\n",
    "print(\"   Purpose: Compare means across multiple groups\")\n",
    "print(\"   Example: Do all product categories have the same average price?\")\n",
    "\n",
    "# Get price data for each category\n",
    "category_groups = []\n",
    "category_names = []\n",
    "for category in ecommerce_data['category'].unique():\n",
    "    category_prices = ecommerce_data[ecommerce_data['category'] == category]['price']\n",
    "    category_groups.append(category_prices.dropna())\n",
    "    category_names.append(category)\n",
    "\n",
    "f_stat, p_value = stats.f_oneway(*category_groups)\n",
    "\n",
    "print(f\"\\n   MULTIPLE GROUP COMPARISON:\")\n",
    "for i, category in enumerate(category_names):\n",
    "    print(f\"   ‚Ä¢ {category} mean price: ${category_groups[i].mean():.2f}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ F-statistic: {f_stat:.4f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"   ‚Ä¢ Result: Significant difference - Product categories have significantly different average prices\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Result: No significant difference - All product categories have similar average prices\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ BUSINESS INSIGHTS FROM STATISTICAL TESTS:\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Age insight\n",
    "if abs(age_data.mean() - target_age) > 2:  # Practical significance\n",
    "    age_direction = \"older\" if age_data.mean() > target_age else \"younger\"\n",
    "    insights.append(f\"Customer base is significantly {age_direction} than expected (avg: {age_data.mean():.1f} vs target: {target_age})\")\n",
    "\n",
    "# Segment insight\n",
    "if premium_spending.mean() > basic_spending.mean() * 1.2:  # 20% higher\n",
    "    insights.append(f\"Premium customers spend {((premium_spending.mean()/basic_spending.mean()-1)*100):.0f}% more than Basic customers\")\n",
    "\n",
    "# Category insight\n",
    "price_ranges = [group.mean() for group in category_groups]\n",
    "if max(price_ranges) > min(price_ranges) * 2:  # 2x difference\n",
    "    highest_cat = category_names[np.argmax(price_ranges)]\n",
    "    lowest_cat = category_names[np.argmin(price_ranges)]\n",
    "    insights.append(f\"{highest_cat} products are much more expensive than {lowest_cat} products\")\n",
    "\n",
    "print(f\"\\n   üí° KEY BUSINESS INSIGHTS:\")\n",
    "for insight in insights:\n",
    "    print(f\"   ‚Ä¢ {insight}\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "---\n",
    "## üé® Part 4: Advanced Visualization for Pattern Discovery\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 4.1 Visualization Strategy for EDA\n",
    "\n",
    "**Why Visualization in EDA?**\n",
    "Visual analysis can reveal patterns, outliers, and relationships that statistical summaries might miss. Different chart types are suited for different types of analysis.\n",
    "\n",
    "#### üìä Visualization Methods for Different Data Types\n",
    "\n",
    "| **Data Type** | **Visualization** | **Purpose** | **When to Use** |\n",
    "|---------------|------------------|-------------|-----------------|\n",
    "| **Single Numerical** | Histogram, Box plot, Violin plot | Distribution analysis | Understand data spread and shape |\n",
    "| **Two Numerical** | Scatter plot, Line plot | Relationship analysis | Find correlations and trends |\n",
    "| **Categorical** | Bar chart, Pie chart | Frequency analysis | Compare categories |\n",
    "| **Numerical + Categorical** | Box plot, Violin plot by group | Group comparisons | Compare distributions across groups |\n",
    "| **Time Series** | Line plot, Area chart | Trend analysis | Identify patterns over time |\n",
    "| **Multiple Variables** | Pair plot, Heatmap | Comprehensive analysis | Overview of many relationships |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Advanced Visualization for Pattern Discovery\n",
    "print(\"üé® ADVANCED VISUALIZATION FOR PATTERN DISCOVERY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ DISTRIBUTION COMPARISON VISUALIZATIONS:\")\n",
    "print(\"   Purpose: Compare distributions across different groups\")\n",
    "\n",
    "# Box plot comparison by segment\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Total Amount by Segment\n",
    "plt.subplot(1, 3, 1)\n",
    "segment_order = ['Basic', 'Standard', 'Premium']  # Order for logical progression\n",
    "box_data = [ecommerce_data[ecommerce_data['segment'] == seg]['total_amount'].dropna() for seg in segment_order]\n",
    "box_plot = plt.boxplot(box_data, labels=segment_order, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.title('Total Amount by Customer Segment')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Total Amount ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Age by City\n",
    "plt.subplot(1, 3, 2)\n",
    "city_order = ['Riyadh', 'Jeddah', 'Dammam', 'Mecca', 'Medina']\n",
    "age_data = [ecommerce_data[ecommerce_data['city'] == city]['age'].dropna() for city in city_order if city in ecommerce_data['city'].unique()]\n",
    "city_labels = [city for city in city_order if city in ecommerce_data['city'].unique()]\n",
    "\n",
    "violin_parts = plt.violinplot(age_data, positions=range(1, len(age_data)+1), showmeans=True)\n",
    "plt.xticks(range(1, len(city_labels)+1), city_labels, rotation=45)\n",
    "plt.title('Age Distribution by City')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Age')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Price by Category\n",
    "plt.subplot(1, 3, 3)\n",
    "categories = ecommerce_data['category'].unique()\n",
    "price_data = [ecommerce_data[ecommerce_data['category'] == cat]['price'].dropna() for cat in categories]\n",
    "\n",
    "box_plot2 = plt.boxplot(price_data, labels=categories, patch_artist=True)\n",
    "colors2 = ['yellow', 'orange', 'lightpink', 'lightcyan', 'lavender']\n",
    "for patch, color in zip(box_plot2['boxes'], colors2[:len(categories)]):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.title('Price Distribution by Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Distribution comparison plots created\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ RELATIONSHIP DISCOVERY VISUALIZATIONS:\")\n",
    "print(\"   Purpose: Identify complex relationships between variables\")\n",
    "\n",
    "# Create advanced relationship plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Advanced Relationship Discovery', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Scatter with categorical coloring\n",
    "scatter = axes[0, 0].scatter(ecommerce_data['age'], ecommerce_data['total_amount'], \n",
    "                           c=ecommerce_data['segment'].astype('category').cat.codes, \n",
    "                           alpha=0.6, cmap='viridis')\n",
    "axes[0, 0].set_xlabel('Customer Age')\n",
    "axes[0, 0].set_ylabel('Total Amount ($)')\n",
    "axes[0, 0].set_title('Age vs Total Amount (colored by Segment)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend for segments\n",
    "unique_segments = ecommerce_data['segment'].unique()\n",
    "for i, segment in enumerate(unique_segments):\n",
    "    axes[0, 0].scatter([], [], c=plt.cm.viridis(i/len(unique_segments)), label=segment)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Hexbin plot for density visualization\n",
    "hexbin = axes[0, 1].hexbin(ecommerce_data['price'], ecommerce_data['total_amount'], \n",
    "                          gridsize=20, cmap='Blues', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Product Price ($)')\n",
    "axes[0, 1].set_ylabel('Total Amount ($)')\n",
    "axes[0, 1].set_title('Price vs Total Amount (Density Plot)')\n",
    "plt.colorbar(hexbin, ax=axes[0, 1])\n",
    "\n",
    "# Plot 3: Bubble plot (3 dimensions)\n",
    "bubble = axes[1, 0].scatter(ecommerce_data['age'], ecommerce_data['total_amount'], \n",
    "                           s=ecommerce_data['quantity']*20, alpha=0.6, \n",
    "                           c=ecommerce_data['price'], cmap='plasma')\n",
    "axes[1, 0].set_xlabel('Customer Age')\n",
    "axes[1, 0].set_ylabel('Total Amount ($)')\n",
    "axes[1, 0].set_title('Age vs Total Amount (size=quantity, color=price)')\n",
    "plt.colorbar(bubble, ax=axes[1, 0])\n",
    "\n",
    "# Plot 4: Correlation with regression line by group\n",
    "for segment in ecommerce_data['segment'].unique():\n",
    "    segment_data = ecommerce_data[ecommerce_data['segment'] == segment]\n",
    "    axes[1, 1].scatter(segment_data['age'], segment_data['total_amount'], \n",
    "                      alpha=0.6, label=segment)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(segment_data['age'], segment_data['total_amount'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1, 1].plot(segment_data['age'], p(segment_data['age']), '--', alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_xlabel('Customer Age')\n",
    "axes[1, 1].set_ylabel('Total Amount ($)')\n",
    "axes[1, 1].set_title('Age vs Total Amount by Segment (with trend lines)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Advanced relationship plots created\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 4.2 Categorical Data Visualization\n",
    "\n",
    "Categorical data requires special visualization techniques to show patterns and relationships effectively.\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "print(\"\\n3Ô∏è‚É£ CATEGORICAL DATA PATTERN ANALYSIS:\")\n",
    "print(\"   Purpose: Discover patterns in categorical variables\")\n",
    "\n",
    "# Create categorical analysis plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Categorical Data Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Count plot by category\n",
    "category_counts = ecommerce_data['category'].value_counts()\n",
    "bars1 = axes[0, 0].bar(category_counts.index, category_counts.values, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('Transaction Count by Category')\n",
    "axes[0, 0].set_xlabel('Product Category')\n",
    "axes[0, 0].set_ylabel('Number of Transactions')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                    f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Segment distribution pie chart\n",
    "segment_counts = ecommerce_data['segment'].value_counts()\n",
    "colors_pie = ['lightcoral', 'lightblue', 'lightgreen']\n",
    "wedges, texts, autotexts = axes[0, 1].pie(segment_counts.values, labels=segment_counts.index, \n",
    "                                          autopct='%1.1f%%', colors=colors_pie)\n",
    "axes[0, 1].set_title('Customer Segment Distribution')\n",
    "\n",
    "# Plot 3: City distribution horizontal bar\n",
    "city_counts = ecommerce_data['city'].value_counts()\n",
    "bars2 = axes[0, 2].barh(city_counts.index, city_counts.values, color='lightgreen', alpha=0.8)\n",
    "axes[0, 2].set_title('Customer Distribution by City')\n",
    "axes[0, 2].set_xlabel('Number of Transactions')\n",
    "axes[0, 2].set_ylabel('City')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    width = bar.get_width()\n",
    "    axes[0, 2].text(width + 50, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{int(width)}', ha='left', va='center')\n",
    "\n",
    "# Plot 4: Stacked bar chart - Category by Segment\n",
    "category_segment_crosstab = pd.crosstab(ecommerce_data['category'], ecommerce_data['segment'])\n",
    "category_segment_crosstab.plot(kind='bar', stacked=True, ax=axes[1, 0], color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "axes[1, 0].set_title('Category Distribution by Segment (Stacked)')\n",
    "axes[1, 0].set_xlabel('Product Category')\n",
    "axes[1, 0].set_ylabel('Number of Transactions')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(title='Segment')\n",
    "\n",
    "# Plot 5: Grouped bar chart - Average spending by category and segment\n",
    "avg_spending = ecommerce_data.groupby(['category', 'segment'])['total_amount'].mean().unstack()\n",
    "avg_spending.plot(kind='bar', ax=axes[1, 1], color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "axes[1, 1].set_title('Average Spending by Category and Segment')\n",
    "axes[1, 1].set_xlabel('Product Category')\n",
    "axes[1, 1].set_ylabel('Average Total Amount ($)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].legend(title='Segment')\n",
    "\n",
    "# Plot 6: Heatmap of cross-tabulation\n",
    "city_category_crosstab = pd.crosstab(ecommerce_data['city'], ecommerce_data['category'])\n",
    "im = axes[1, 2].imshow(city_category_crosstab.values, cmap='Blues', aspect='auto')\n",
    "axes[1, 2].set_xticks(range(len(city_category_crosstab.columns)))\n",
    "axes[1, 2].set_yticks(range(len(city_category_crosstab.index)))\n",
    "axes[1, 2].set_xticklabels(city_category_crosstab.columns, rotation=45)\n",
    "axes[1, 2].set_yticklabels(city_category_crosstab.index)\n",
    "axes[1, 2].set_title('City vs Category Heatmap')\n",
    "axes[1, 2].set_xlabel('Product Category')\n",
    "axes[1, 2].set_ylabel('City')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=axes[1, 2])\n",
    "\n",
    "# Add text annotations to heatmap\n",
    "for i in range(len(city_category_crosstab.index)):\n",
    "    for j in range(len(city_category_crosstab.columns)):\n",
    "        text = axes[1, 2].text(j, i, city_category_crosstab.iloc[i, j],\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Categorical pattern analysis completed\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ KEY PATTERNS DISCOVERED:\")\n",
    "\n",
    "# Analyze the patterns\n",
    "most_popular_category = ecommerce_data['category'].value_counts().index[0]\n",
    "most_popular_count = ecommerce_data['category'].value_counts().iloc[0]\n",
    "\n",
    "largest_segment = ecommerce_data['segment'].value_counts().index[0]\n",
    "largest_segment_pct = (ecommerce_data['segment'].value_counts().iloc[0] / len(ecommerce_data)) * 100\n",
    "\n",
    "most_active_city = ecommerce_data['city'].value_counts().index[0]\n",
    "most_active_count = ecommerce_data['city'].value_counts().iloc[0]\n",
    "\n",
    "print(f\"   üõçÔ∏è Most popular category: {most_popular_category} ({most_popular_count:,} transactions)\")\n",
    "print(f\"   üë• Largest customer segment: {largest_segment} ({largest_segment_pct:.1f}% of customers)\")\n",
    "print(f\"   üèôÔ∏è Most active city: {most_active_city} ({most_active_count:,} transactions)\")\n",
    "\n",
    "# Cross-category insights\n",
    "highest_spending_combo = avg_spending.stack().idxmax()\n",
    "highest_spending_amount = avg_spending.stack().max()\n",
    "category, segment = highest_spending_combo\n",
    "print(f\"   üí∞ Highest spending combination: {segment} customers buying {category} (${highest_spending_amount:.2f} average)\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "---\n",
    "## ‚è∞ Part 5: Time Series Analysis and Trends\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 5.1 Understanding Time Series Data\n",
    "\n",
    "**What is Time Series Analysis?**\n",
    "Time series analysis examines data points collected over time to identify trends, seasonal patterns, and cyclical behaviors. This is crucial for business forecasting and understanding customer behavior patterns.\n",
    "\n",
    "#### üìä Time Series Analysis Components\n",
    "\n",
    "| **Component** | **Description** | **Business Example** | **Analysis Method** |\n",
    "|---------------|-----------------|---------------------|-------------------|\n",
    "| **Trend** | Long-term increase or decrease | Growing sales over years | Linear regression, moving averages |\n",
    "| **Seasonality** | Regular patterns that repeat | Higher sales in holidays | Seasonal decomposition |\n",
    "| **Cyclical** | Long-term fluctuations | Economic cycles | Cycle analysis |\n",
    "| **Irregular** | Random fluctuations | Unexpected events | Outlier detection |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Time Series Analysis\n",
    "print(\"‚è∞ TIME SERIES ANALYSIS AND TRENDS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ PREPARING TIME SERIES DATA:\")\n",
    "print(\"   Purpose: Aggregate transaction data by time periods\")\n",
    "\n",
    "# Create time-based aggregations\n",
    "ecommerce_data['transaction_date'] = pd.to_datetime(ecommerce_data['transaction_date'])\n",
    "\n",
    "# Daily aggregations\n",
    "daily_sales = ecommerce_data.groupby(ecommerce_data['transaction_date'].dt.date).agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "daily_sales.columns = ['daily_revenue', 'avg_transaction', 'transaction_count', 'total_quantity']\n",
    "daily_sales.reset_index(inplace=True)\n",
    "daily_sales['transaction_date'] = pd.to_datetime(daily_sales['transaction_date'])\n",
    "\n",
    "print(f\"   ‚úÖ Created daily sales data: {len(daily_sales)} days\")\n",
    "\n",
    "# Monthly aggregations\n",
    "monthly_sales = ecommerce_data.groupby([\n",
    "    ecommerce_data['transaction_date'].dt.year, \n",
    "    ecommerce_data['transaction_date'].dt.month\n",
    "]).agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "monthly_sales.columns = ['monthly_revenue', 'avg_transaction', 'transaction_count', 'total_quantity']\n",
    "monthly_sales.reset_index(inplace=True)\n",
    "monthly_sales['date'] = pd.to_datetime(monthly_sales[['transaction_date', 'level_1']].rename(columns={'level_1': 'month'}))\n",
    "\n",
    "print(f\"   ‚úÖ Created monthly sales data: {len(monthly_sales)} months\")\n",
    "\n",
    "# Weekly patterns\n",
    "weekly_pattern = ecommerce_data.groupby(ecommerce_data['transaction_date'].dt.day_name()).agg({\n",
    "    'total_amount': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "weekly_pattern.columns = ['weekly_revenue', 'avg_transaction', 'transaction_count']\n",
    "\n",
    "# Reorder days\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_pattern = weekly_pattern.reindex(day_order)\n",
    "\n",
    "print(f\"   ‚úÖ Created weekly pattern analysis\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ TREND ANALYSIS:\")\n",
    "print(\"   Purpose: Identify long-term patterns in the data\")\n",
    "\n",
    "# Create trend visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Time Series Analysis - Trends and Patterns', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Daily revenue trend\n",
    "axes[0, 0].plot(daily_sales['transaction_date'], daily_sales['daily_revenue'], \n",
    "                color='blue', alpha=0.7, linewidth=1)\n",
    "\n",
    "# Add moving average\n",
    "daily_sales['ma_7'] = daily_sales['daily_revenue'].rolling(window=7).mean()\n",
    "daily_sales['ma_30'] = daily_sales['daily_revenue'].rolling(window=30).mean()\n",
    "\n",
    "axes[0, 0].plot(daily_sales['transaction_date'], daily_sales['ma_7'], \n",
    "                color='red', linewidth=2, label='7-day Moving Average')\n",
    "axes[0, 0].plot(daily_sales['transaction_date'], daily_sales['ma_30'], \n",
    "                color='green', linewidth=2, label='30-day Moving Average')\n",
    "\n",
    "axes[0, 0].set_title('Daily Revenue Trend')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Daily Revenue ($)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Monthly revenue trend\n",
    "axes[0, 1].plot(monthly_sales['date'], monthly_sales['monthly_revenue'], \n",
    "                marker='o', color='green', linewidth=2, markersize=6)\n",
    "axes[0, 1].set_title('Monthly Revenue Trend')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Monthly Revenue ($)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(range(len(monthly_sales)), monthly_sales['monthly_revenue'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 1].plot(monthly_sales['date'], p(range(len(monthly_sales))), \n",
    "                'r--', alpha=0.8, label=f'Trend Line')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Weekly pattern\n",
    "bars = axes[1, 0].bar(weekly_pattern.index, weekly_pattern['weekly_revenue'], \n",
    "                      color='skyblue', alpha=0.8)\n",
    "axes[1, 0].set_title('Weekly Revenue Pattern')\n",
    "axes[1, 0].set_xlabel('Day of Week')\n",
    "axes[1, 0].set_ylabel('Total Revenue ($)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'${height:,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 4: Hourly pattern\n",
    "hourly_pattern = ecommerce_data.groupby(ecommerce_data['hour'])['total_amount'].sum()\n",
    "axes[1, 1].plot(hourly_pattern.index, hourly_pattern.values, \n",
    "                marker='o', color='orange', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_title('Hourly Sales Pattern')\n",
    "axes[1, 1].set_xlabel('Hour of Day')\n",
    "axes[1, 1].set_ylabel('Total Revenue ($)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"   ‚úÖ Trend analysis visualizations created\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ SEASONAL PATTERN ANALYSIS:\")\n",
    "print(\"   Purpose: Identify recurring patterns and seasonality\")\n",
    "\n",
    "# Monthly pattern analysis\n",
    "monthly_pattern = ecommerce_data.groupby(ecommerce_data['month'])['total_amount'].agg(['sum', 'mean', 'count'])\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "print(\"\\n   üìä MONTHLY SEASONAL PATTERNS:\")\n",
    "print(\"   Month | Total Revenue | Avg Transaction | Count\")\n",
    "print(\"   ------|---------------|-----------------|------\")\n",
    "for month in monthly_pattern.index:\n",
    "    total_rev = monthly_pattern.loc[month, 'sum']\n",
    "    avg_trans = monthly_pattern.loc[month, 'mean']\n",
    "    count = monthly_pattern.loc[month, 'count']\n",
    "    month_name = month_names[month-1]\n",
    "    print(f\"   {month_name:5} | ${total_rev:12,.0f} | ${avg_trans:14.2f} | {count:5.0f}\")\n",
    "\n",
    "# Weekly pattern analysis\n",
    "print(\"\\n   üìä WEEKLY SEASONAL PATTERNS:\")\n",
    "print(\"   Day       | Total Revenue | Avg Transaction | Count\")\n",
    "print(\"   ----------|---------------|-----------------|------\")\n",
    "for day in weekly_pattern.index:\n",
    "    total_rev = weekly_pattern.loc[day, 'weekly_revenue']\n",
    "    avg_trans = weekly_pattern.loc[day, 'avg_transaction']\n",
    "    count = weekly_pattern.loc[day, 'transaction_count']\n",
    "    print(f\"   {day:9} | ${total_rev:12,.0f} | ${avg_trans:14.2f} | {count:5.0f}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ TREND INSIGHTS AND BUSINESS IMPLICATIONS:\")\n",
    "\n",
    "# Calculate trend metrics\n",
    "overall_trend = np.polyfit(range(len(daily_sales)), daily_sales['daily_revenue'], 1)[0]\n",
    "recent_avg = daily_sales['daily_revenue'].tail(30).mean()\n",
    "early_avg = daily_sales['daily_revenue'].head(30).mean()\n",
    "\n",
    "print(f\"\\n   üìà TREND ANALYSIS:\")\n",
    "if overall_trend > 0:\n",
    "    print(f\"   ‚Ä¢ Overall trend: Positive (${overall_trend:.2f} per day increase)\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Overall trend: Negative (${abs(overall_trend):.2f} per day decrease)\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Recent 30-day average: ${recent_avg:.2f}\")\n",
    "print(f\"   ‚Ä¢ Early 30-day average: ${early_avg:.2f}\")\n",
    "print(f\"   ‚Ä¢ Growth rate: {((recent_avg/early_avg-1)*100):+.1f}%\")\n",
    "\n",
    "# Best and worst performing days/months\n",
    "best_day = weekly_pattern['weekly_revenue'].idxmax()\n",
    "worst_day = weekly_pattern['weekly_revenue'].idxmin()\n",
    "best_month = monthly_pattern['sum'].idxmax()\n",
    "worst_month = monthly_pattern['sum'].idxmin()\n",
    "\n",
    "print(f\"\\n   üéØ PERFORMANCE INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Best performing day: {best_day} (${weekly_pattern.loc[best_day, 'weekly_revenue']:,.0f})\")\n",
    "print(f\"   ‚Ä¢ Worst performing day: {worst_day} (${weekly_pattern.loc[worst_day, 'weekly_revenue']:,.0f})\")\n",
    "print(f\"   ‚Ä¢ Best performing month: {month_names[best_month-1]} (${monthly_pattern.loc[best_month, 'sum']:,.0f})\")\n",
    "print(f\"   ‚Ä¢ Worst performing month: {month_names[worst_month-1]} (${monthly_pattern.loc[worst_month, 'sum']:,.0f})\")\n",
    "\n",
    "# Hour pattern insights\n",
    "peak_hour = hourly_pattern.idxmax()\n",
    "low_hour = hourly_pattern.idxmin()\n",
    "print(f\"   ‚Ä¢ Peak sales hour: {peak_hour}:00 (${hourly_pattern[peak_hour]:,.0f})\")\n",
    "print(f\"   ‚Ä¢ Lowest sales hour: {low_hour}:00 (${hourly_pattern[low_hour]:,.0f})\")\n",
    "```\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "---\n",
    "## üíº Part 6: Comparative Analysis and Business Insights\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "### 6.1 Comparative Analysis Techniques\n",
    "\n",
    "**What is Comparative Analysis?**\n",
    "Comparative analysis involves comparing different groups, time periods, or segments to identify differences and opportunities for improvement.\n",
    "\n",
    "#### üìä Comparative Analysis Methods\n",
    "\n",
    "| **Comparison Type** | **Purpose** | **Methods** | **Business Applications** |\n",
    "|-------------------|-------------|-------------|---------------------------|\n",
    "| **Segment Comparison** | Compare customer groups | Mean comparison, statistical tests | Marketing strategy, pricing |\n",
    "| **Time Period Comparison** | Compare different periods | Year-over-year, period-over-period | Performance tracking, trends |\n",
    "| **Geographic Comparison** | Compare locations | Regional analysis | Expansion planning, localization |\n",
    "| **Product Comparison** | Compare product performance | Category analysis | Inventory management, promotion |\n",
    "\n",
    "<!-- CELL BREAK -->\n",
    "\n",
    "```python\n",
    "# Comparative Analysis and Business Insights\n",
    "print(\"üíº COMPARATIVE ANALYSIS AND BUSINESS INSIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ CUSTOMER SEGMENT PERFORMANCE COMPARISON:\")\n",
    "print(\"   Purpose: Identify highest value customer segments\")\n",
    "\n",
    "# Comprehensive segment analysis\n",
    "segment_analysis = ecommerce_data.groupby('segment').agg({\n",
    "    'total_amount': ['sum', 'mean', 'median', 'count'],\n",
    "    'quantity': ['sum', 'mean'],\n",
    "    'age': 'mean',\n",
    "    'customer_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "segment_analysis.columns = ['total_revenue', 'avg_order_value', 'median_order_value', 'order_count', \n",
    "                          'total_quantity', 'avg_quantity', 'avg_age', 'unique_customers']\n",
    "\n",
    "# Calculate additional metrics\n",
    "segment_analysis['revenue_per_customer'] = segment_analysis['total_revenue'] / segment_analysis['unique_customers']\n",
    "segment_analysis['orders_per_customer'] = segment_analysis['order_count'] / segment_analysis['unique_customers']\n",
    "\n",
    "print(\"\\n   üìä SEGMENT PERFORMANCE METRICS:\")\n",
    "print(segment_analysis)\n",
    "\n",
    "# Find top performing segments\n",
    "top_revenue_segment = segment_analysis['total_revenue'].idxmax()\n",
    "top_aov_segment = segment_analysis['avg_order_value'].idxmax()\n",
    "top_loyalty_segment = segment_analysis['orders_per_customer'].idxmax()\n",
    "\n",
    "print(f\"\\n   üèÜ TOP PERFORMING SEGMENTS:\")\n",
    "print(f\"   ‚Ä¢ Highest total revenue: {top_revenue_segment} (${segment_analysis.loc[top_revenue_segment, 'total_revenue']:,.0f})\")\n",
    "print(f\"   ‚Ä¢ Highest average order value: {top_aov_segment} (${segment_analysis.loc[top_aov_segment, 'avg_order_value']:.2f})\")\n",
    "print(f\"   ‚Ä¢ Most loyal customers: {top_loyalty_segment} ({segment_analysis.loc[top_loyalty_segment, 'orders_per_customer']:.1f} orders/customer)\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ GEOGRAPHIC PERFORMANCE COMPARISON:\")\n",
    "print(\"   Purpose: Identify best performing cities and expansion opportunities\")\n",
    "\n",
    "# Geographic analysis\n",
    "city_analysis = ecommerce_data.groupby('city').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'customer_id': 'nunique',\n",
    "    'age': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "city_analysis.columns = ['total_revenue', 'avg_order_value', 'order_count', 'unique_customers', 'avg_age']\n",
    "city_analysis['revenue_per_customer'] = city_analysis['total_revenue'] / city_analysis['unique_customers']\n",
    "city_analysis['market_share'] = (city_analysis['total_revenue'] / city_analysis['total_revenue'].sum()) * 100\n",
    "\n",
    "print(\"\\n   üìä CITY PERFORMANCE METRICS:\")\n",
    "print(city_analysis)\n",
    "\n",
    "# Ranking cities\n",
    "print(f\"\\n   üèÜ CITY RANKINGS:\")\n",
    "print(f\"   ‚Ä¢ By total revenue: {city_analysis['total_revenue'].sort_values(ascending=False).index.tolist()}\")\n",
    "print(f\"   ‚Ä¢ By average order value: {city_analysis['avg_order_value'].sort_values(ascending=False).index.tolist()}\")\n",
    "print(f\"   ‚Ä¢ By revenue per customer: {city_analysis['revenue_per_customer'].sort_values(ascending=False).index.tolist()}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ PRODUCT CATEGORY PERFORMANCE COMPARISON:\")\n",
    "print(\"   Purpose: Identify top performing product categories\")\n",
    "\n",
    "# Category analysis\n",
    "category_analysis = ecommerce_data.groupby('category').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'price': ['mean', 'median'],\n",
    "    'quantity': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "category_analysis.columns = ['total_revenue', 'avg_order_value', 'order_count', \n",
    "                           'avg_price', 'median_price', 'total_quantity', 'avg_quantity']\n",
    "category_analysis['market_share'] = (category_analysis['total_revenue'] / category_analysis['total_revenue'].sum()) * 100\n",
    "\n",
    "print(\"\\n   üìä CATEGORY PERFORMANCE METRICS:\")\n",
    "print(category_analysis)\n",
    "\n",
    "# Category insights\n",
    "best_revenue_category = category_analysis['total_revenue'].idxmax()\n",
    "highest_price_category = category_analysis['avg_price'].idxmax()\n",
    "most_ordered_category = category_analysis['order_count'].idxmax()\n",
    "\n",
    "print(f\"\\n   üèÜ CATEGORY INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Highest revenue category: {best_revenue_category} (${category_analysis.loc[best_revenue_category, 'total_revenue']:,.0f})\")\n",
    "print(f\"   ‚Ä¢ Highest priced category: {highest_price_category} (${category_analysis.loc[highest_price_category, 'avg_price']:.2f} avg price)\")\n",
    "print(f\"   ‚Ä¢ Most frequently ordered: {most_ordered_category} ({category_analysis.loc[most_ordered_category, 'order_count']:,.0f} orders)\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ TIME-BASED PERFORMANCE COMPARISON:\")\n",
    "print(\"   Purpose: Compare performance across different time periods\")\n",
    "\n",
    "# Create quarterly comparison\n",
    "ecommerce_data['quarter'] = ecommerce_data['transaction_date'].dt.quarter\n",
    "quarterly_performance = ecommerce_data.groupby('quarter')['total_amount'].agg(['sum', 'mean', 'count'])\n",
    "\n",
    "print(\"\\n   üìä QUARTERLY PERFORMANCE:\")\n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "for q in quarterly_performance.index:\n",
    "    if q <= len(quarters):\n",
    "        quarter_name = quarters[q-1]\n",
    "        revenue = quarterly_performance.loc[q, 'sum']\n",
    "        avg_order = quarterly_performance.loc[q, 'mean']\n",
    "        count = quarterly_performance.loc[q, 'count']\n",
    "        print(f\"   ‚Ä¢ {quarter_name}: ${revenue:,.0f} total, ${avg_order:.2f} avg, {count:.0f} orders\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ COMPREHENSIVE BUSINESS INSIGHTS:\")\n",
    "\n",
    "# Create executive summary insights\n",
    "insights = []\n",
    "\n",
    "# Revenue insights\n",
    "total_revenue = ecommerce_data['total_amount'].sum()\n",
    "total_customers = ecommerce_data['customer_id'].nunique()\n",
    "total_orders = len(ecommerce_data)\n",
    "avg_customer_value = total_revenue / total_customers\n",
    "\n",
    "insights.append(f\"üí∞ Total business performance: ${total_revenue:,.0f} revenue from {total_customers:,} customers across {total_orders:,} orders\")\n",
    "insights.append(f\"üìà Average customer value: ${avg_customer_value:.2f}\")\n",
    "\n",
    "# Segment insights\n",
    "premium_share = (segment_analysis.loc['Premium', 'total_revenue'] / total_revenue) * 100\n",
    "insights.append(f\"üíé Premium customers represent {premium_share:.1f}%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
